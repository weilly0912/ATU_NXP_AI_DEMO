Warning: Unsupported TensorFlow Lite semantics for QUANTIZE 'sub_7_int8'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: sub_7
Warning: Unsupported TensorFlow Lite semantics for DEQUANTIZE 'ResizeBilinear_2'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: ResizeBilinear_2
Warning: AVERAGE_POOL_2D 'AvgPool2D/AvgPool' is not supported on the NPU. Placing on CPU instead
 - Stride values for both width and height must be in the range [1, 3]
   Op has stride WxH as: 65x65
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_16/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_16/depthwise/depthwise2' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_16/depthwise/depthwise/SpaceToBatchND' has batch size: 16
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_16/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_13/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_13/depthwise/depthwise2' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_13/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_13/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_10/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_10/depthwise/depthwise2' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_10/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_10/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_7/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_7/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_7/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_7/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_8/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_8/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_8/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_8/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_9/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_9/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_9/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_9/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_11/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_11/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_11/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_11/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_12/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_12/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_12/depthwise/depthwise/SpaceToBatchND' has batch size: 4
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_12/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_14/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_14/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_14/depthwise/depthwise/SpaceToBatchND' has batch size: 16
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_14/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Info: BATCH_TO_SPACE_ND 'MobilenetV2/expanded_conv_15/depthwise/depthwise/BatchToSpaceND' is a CPU only op
Warning: DEPTHWISE_CONV_2D 'MobilenetV2/expanded_conv_15/depthwise/depthwise1' is not supported on the NPU. Placing on CPU instead
 - IFM Tensor batch size must be 1
   Tensor 'MobilenetV2/expanded_conv_15/depthwise/depthwise/SpaceToBatchND' has batch size: 16
Info: SPACE_TO_BATCH_ND 'MobilenetV2/expanded_conv_15/depthwise/depthwise/SpaceToBatchND' is a CPU only op
Warning: AvgPool operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: DepthwiseConv2DBias operation is unknown or unsupported, placing on CPU
Warning: Quantize operation is unknown or unsupported, placing on CPU

Network summary for lite-model_deeplabv3-mobilenetv2-int8_1_default_1_quant
Accelerator configuration               Ethos_U65_256
System configuration                 internal-default
Memory mode                          internal-default
Accelerator clock                                1000 MHz
Design peak SRAM bandwidth                      16.00 GB/s
Design peak DRAM bandwidth                       3.75 GB/s

Total SRAM used                                381.70 KiB
Total DRAM used                              11874.17 KiB

CPU operators = 13 (14.0%)
NPU operators = 80 (86.0%)

Average SRAM bandwidth                           1.10 GB/s
Input   SRAM bandwidth                         117.41 MB/batch
Weight  SRAM bandwidth                         122.12 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                         239.62 MB/batch
Total   SRAM bandwidth            per input    239.62 MB/inference (batch size 1)

Average DRAM bandwidth                           1.45 GB/s
Input   DRAM bandwidth                         171.24 MB/batch
Weight  DRAM bandwidth                          21.20 MB/batch
Output  DRAM bandwidth                         123.43 MB/batch
Total   DRAM bandwidth                         315.88 MB/batch
Total   DRAM bandwidth            per input    315.88 MB/inference (batch size 1)

Neural network macs                        8597636032 MACs/batch
Network Tops/s                                   0.08 Tops/s

NPU cycles                                   94363161 cycles/batch
SRAM Access cycles                           11978940 cycles/batch
DRAM Access cycles                          198721829 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                        0 cycles/batch
Total cycles                                217437178 cycles/batch

Batch Inference time               217.44 ms,    4.60 inferences/s (batch size 1)

